{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15714\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\15714\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for processing\n",
    "from collections import defaultdict  # for word frequency\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from time import time\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import spacy  # for preprocessing\n",
    "import en_core_web_sm\n",
    "\n",
    "# for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics\n",
    "\n",
    "# for explainer\n",
    "from lime import lime_text\n",
    "\n",
    "# for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging  # setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "#https://spacy.io/usage/linguistic-features\n",
    "    #Text: The original word text.\n",
    "    #Lemma: The base form of the word.\n",
    "    #POS: The simple UPOS part-of-speech tag.\n",
    "    #Tag: The detailed part-of-speech tag.\n",
    "    #Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "    #Shape: The word shape â€“ capitalization, punctuation, digits.\n",
    "    #is alpha: Is the token an alpha character?\n",
    "    #is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "    \n",
    "# re - regex sub    \n",
    "# brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf (advanced variant of BoW)\n",
    "# compare with:\n",
    "# elMo\n",
    "# fasttext\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_train[\"text_clean\"]\n",
    "vectorizer.fit(corpus)\n",
    "X_train = vectorizer.transform(corpus)\n",
    "dic_vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep statistically significant words from chi-square test\n",
    "\n",
    "y = df_train[\"real\"]\n",
    "X_names = vectorizer.get_feature_names()\n",
    "p_value_limit = 0.95\n",
    "dtf_features = pd.DataFrame()\n",
    "for cat in np.unique(y):\n",
    "    chi2, p = feature_selection.chi2(X_train, y==cat)\n",
    "    dtf_features = dtf_features.append(pd.DataFrame(\n",
    "                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n",
    "    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n",
    "                    ascending=[True,False])\n",
    "    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n",
    "X_names = dtf_features[\"feature\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take top 2000-7000 words and use as training set\n",
    "X_names[:5]\n",
    "len(X_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "# pipeline\n",
    "model = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n",
    "                           (\"classifier\", classifier)])\n",
    "# train classifier\n",
    "model[\"classifier\"].fit(X_train, y_train)\n",
    "# test\n",
    "X_test = df_test[\"text_clean\"].values\n",
    "predicted = model.predict(X_test)\n",
    "predicted_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "# Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "auc = metrics.roc_auc_score(y_test, predicted, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "    \n",
    "# Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solis",
   "language": "python",
   "name": "solis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
